[Tests]
  design = 'MooseException.md'
  issues = '#3777 #9181'
  # This example throws an exception during computeResidual() in the
  # first timestep, and then continues running with a reduced
  # timestep.  This example requires PETSc >= 3.6, otherwise you get:
  # PETSC ERROR: Petsc has generated inconsistent data
  # PETSC ERROR: Computed Nan differencing parameter h
  [./parallel_exception_residual_transient]
    type = 'Exodiff'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    exodiff = 'parallel_exception_residual_transient_out.e'
    use_old_floor = true

    requirement = 'The system shall support throwing an exception during the residual '
                  'calculation, which will cut back the time step.'
  [../]

  [./parallel_exception_residual_transient_non_zero_rank]
    type = 'Exodiff'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    cli_args = 'Kernels/exception/rank=1'
    exodiff = 'parallel_exception_residual_transient_out.e'
    use_old_floor = true
    prereq = 'parallel_exception_residual_transient'
    min_parallel = 2
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10

    requirement = 'The system shall support throwing an exception during the residual '
                  'calculation on a non-zero rank, which will cut back the time step.'
  [../]

  # This example throws an exception during computeJacobian() in the
  # first timestep, and then continues running with a reduced
  # timestep.
  [./parallel_exception_jacobian_transient]
    type = 'Exodiff'
    input = 'parallel_exception_jacobian_transient.i'
    petsc_version = '>=3.6.0'
    exodiff = 'parallel_exception_jacobian_transient_out.e'
    use_old_floor = true

    requirement = 'The system shall support throwing an exception during the Jacboain '
                  'calculation, which will cut back the time step.'
  [../]

  [./parallel_exception_jacobian_transient_non_zero_rank]
    type = 'Exodiff'
    input = 'parallel_exception_jacobian_transient.i'
    petsc_version = '>=3.6.0'
    cli_args = 'Kernels/exception/rank=1'
    exodiff = 'parallel_exception_jacobian_transient_out.e'
    use_old_floor = true
    prereq = 'parallel_exception_jacobian_transient'
    min_parallel = 2
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10

    requirement = 'The system shall support throwing an exception during the Jacobian '
                  'calculation on a non-zero rank, which will cut back the time step.'
  [../]

  [./parallel_exception_initial_condition]
    type = RunApp
    input = 'parallel_exception_initial_condition.i'
    petsc_version = '>=3.5.0'
    expect_out = 'MooseException thrown during initial condition computation'

    requirement = 'The system shall support throwing an exception during the initial condition '
                  'calculation, which will terminate the solve.'
  [../]

  # Test that we can trigger an error on a non-zero rank and exit as expected
  [./parallel_error_residual_transient_non_zero_rank]
    type = 'RunException'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    min_parallel = 2
    cli_args = 'Kernels/exception/rank=1 Kernels/exception/should_throw=false Outputs/exodus=false'
    expect_err = 'Intentional error triggered during residual calculation'
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10

    requirement = 'The system shall support throwing an error during a residual '
                  'calculation, which will terminate the solve.'
  [../]

  [./parallel_error_jacobian_transient_non_zero_rank]
    type = 'RunException'
    input = 'parallel_exception_residual_transient.i'
    petsc_version = '>=3.6.0'
    min_parallel = 2
    cli_args = 'Kernels/exception/rank=1 Kernels/exception/should_throw=false Outputs/exodus=false'
    expect_err = 'Intentional error triggered during residual calculation'
    # The mesh is too small, and the number of elements on MPI rank 1 is 0 when we are uing 16 MPI processes. This happens with PETSc-3.8.3
    max_parallel = 10

    requirement = 'The system shall support throwing an error during a Jacobian '
                  'calculation, which will terminate the solve.'
  [../]

  [./skip_exception_check]
    type = 'Exodiff'
    input = '2d_diffusion_skip_exception.i'
    exodiff = '2d_diffusion_skip_exception_out.e'
    expect_out = 'MOOSE may fail to catch an exception when the "skip_exception_check" parameter is used. If you receive a terse MPI error during execution, remove this parameter and rerun your simulation'
    allow_warnings = true
    requirement = "The system shall allow users to skip exception checks to avoid global communication."
  [../]
[]
